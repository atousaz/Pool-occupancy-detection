{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odGqTQeu0yyR",
        "outputId": "9cb184d7-02ec-4803-9ba5-8026d51c526e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define paths\n",
        "input_dirs = [\n",
        "    '/content/drive/My Drive/GAN/Real',\n",
        "    '/content/drive/My Drive/Dall_E',\n",
        "    '/content/drive/My Drive/stable_difuser_images'\n",
        "]\n",
        "label_folder = '/content/drive/My Drive/processed_images/centerbased/labels'\n",
        "center_based_model_folder = '/content/drive/My Drive/processed_images/centerbased'"
      ],
      "metadata": {
        "id": "pD4a5Tso8Gd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok0OlXSg06M7",
        "outputId": "b89a540f-510f-45ea-f7ab-b54820f16786"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:293: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0'] not found, attempting AutoUpdate...\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 8.5 MB/s eta 0:00:00\n",
            "Collecting pillow>=10.3.0\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 130.5 MB/s eta 0:00:00\n",
            "Collecting requests>=2.32.0\n",
            "  Downloading requests-2.32.2-py3-none-any.whl (63 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.9/63.9 kB 273.4 MB/s eta 0:00:00\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 273.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, requests, pillow, gitdb, gitpython\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pillow-10.3.0 requests-2.32.2 smmap-5.0.1\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 8.5s, installed 3 packages: ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 🚀 2024-5-25 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt to yolov5l.pt...\n",
            "100%|██████████| 89.3M/89.3M [00:00<00:00, 444MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "# Load YOLOv5 large model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5l')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvPPGkC28a_Y"
      },
      "source": [
        "# center based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbvwOLs20z-B",
        "outputId": "44192819-d5ed-4b40-a857-b1f0fd6d1aee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing images in /content/drive/My Drive/GAN/Real: 100%|██████████| 372/372 [10:43<00:00,  1.73s/it]\n",
            "Processing images in /content/drive/My Drive/Dall_E: 100%|██████████| 131/131 [05:02<00:00,  2.31s/it]\n",
            "Processing images in /content/drive/My Drive/stable_difuser_images: 100%|██████████| 1064/1064 [39:35<00:00,  2.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images processed: 1567\n",
            "Total humans detected: 1950\n",
            "Total chairs detected: 9614\n",
            "Total humans on chairs detected: 205\n",
            "Processing completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define paths for each input directory\n",
        "input_dirs = {\n",
        "    'GAN/Real': '/content/drive/My Drive/GAN/Real',\n",
        "    'Dall_E': '/content/drive/My Drive/Dall_E',\n",
        "    'Stable_Diffuser': '/content/drive/My Drive/stable_difuser_images'\n",
        "}\n",
        "label_folder = '/content/drive/My Drive/processed_images/centerbased/labels'\n",
        "center_based_model_folder = '/content/drive/My Drive/processed_images/centerbased'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "images_with_human_on_chair_folder = os.path.join(center_based_model_folder, 'images_with_human_on_chair')\n",
        "annotations_with_human_on_chair_folder = os.path.join(center_based_model_folder, 'annotations_with_human_on_chair')\n",
        "\n",
        "os.makedirs(images_with_human_on_chair_folder, exist_ok=True)\n",
        "os.makedirs(annotations_with_human_on_chair_folder, exist_ok=True)\n",
        "\n",
        "# Define relevant classes for objects that a human can sit on next to a pool\n",
        "relevant_classes = [0, 56, 57, 58]  # Include humans and objects they can sit on\n",
        "\n",
        "# Function to convert YOLO bbox to normal bbox\n",
        "def yolo_to_bbox(yolo_bbox, img_width, img_height):\n",
        "    x_center, y_center, width, height = yolo_bbox\n",
        "    x = int((x_center - width / 2) * img_width)\n",
        "    y = int((y_center - height / 2) * img_height)\n",
        "    w = int(width * img_width)\n",
        "    h = int(height * img_height)\n",
        "    return [x, y, w, h]\n",
        "\n",
        "# Function to convert normal bbox to YOLO bbox\n",
        "def bbox_to_yolo(bbox, img_width, img_height):\n",
        "    x, y, w, h = bbox\n",
        "    x_center = (x + w / 2) / img_width\n",
        "    y_center = (y + h / 2) / img_height\n",
        "    width = w / img_width\n",
        "    height = h / img_height\n",
        "    return [x_center, y_center, width, height]\n",
        "\n",
        "# Function to check if a human is on a chair\n",
        "def is_human_on_chair(human_bbox, chair_bbox):\n",
        "    hx, hy, hw, hh = human_bbox\n",
        "    cx, cy, cw, ch = chair_bbox\n",
        "    # Check if the center of the human is within the chair bounding box\n",
        "    return (cx <= hx + hw / 2 <= cx + cw) and (cy <= hy + hh / 2 <= cy + ch)\n",
        "\n",
        "# Function to detect objects using YOLOv5 model\n",
        "def detect_objects(image_path):\n",
        "    results = model(image_path)\n",
        "    return results.xyxy[0].cpu().numpy()\n",
        "\n",
        "# Function to plot bounding boxes on an example image and save it\n",
        "def plot_and_save_image_with_bboxes(image_path):\n",
        "    image_name = os.path.basename(image_path)\n",
        "    label_path = os.path.join(label_folder, image_name.replace('.jpg', '.txt'))\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Failed to read image: {image_path}\")\n",
        "        return 0, 0, 0\n",
        "\n",
        "    img_height, img_width = img.shape[:2]\n",
        "\n",
        "    detections = detect_objects(image_path)\n",
        "    humans = []\n",
        "    chairs = []\n",
        "\n",
        "    for det in detections:\n",
        "        class_id, x1, y1, x2, y2, confidence = int(det[5]), int(det[0]), int(det[1]), int(det[2]), int(det[3]), float(det[4])\n",
        "        bbox = [x1, y1, x2 - x1, y2 - y1]\n",
        "        if class_id == 0:\n",
        "            humans.append(bbox)\n",
        "        elif class_id in relevant_classes:\n",
        "            chairs.append(bbox)\n",
        "\n",
        "    human_on_chair_count = 0\n",
        "    new_annotations = []\n",
        "\n",
        "    for human_bbox in humans:\n",
        "        for chair_bbox in chairs:\n",
        "            if is_human_on_chair(human_bbox, chair_bbox):\n",
        "                human_on_chair_count += 1\n",
        "                cv2.rectangle(img, (human_bbox[0], human_bbox[1]), (human_bbox[0] + human_bbox[2], human_bbox[1] + human_bbox[3]), (255, 0, 0), 3)\n",
        "                cv2.putText(img, 'Human on Chair', (human_bbox[0], human_bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "                # Save the new annotation for human on chair\n",
        "                new_annotations.append((2, bbox_to_yolo(human_bbox, img_width, img_height)))\n",
        "                break\n",
        "\n",
        "    if human_on_chair_count > 0:\n",
        "        # Save the image with bounding boxes and annotations\n",
        "        save_path = os.path.join(images_with_human_on_chair_folder, 'human_on_chair_' + image_name)\n",
        "        cv2.imwrite(save_path, img)\n",
        "\n",
        "        # Save the new annotations for human on chair\n",
        "        annotation_save_path = os.path.join(annotations_with_human_on_chair_folder, image_name.replace('.jpg', '.txt'))\n",
        "        with open(annotation_save_path, 'w') as f:\n",
        "            for class_id, yolo_bbox in new_annotations:\n",
        "                f.write(f\"{class_id} {' '.join(map(str, yolo_bbox))}\\n\")\n",
        "\n",
        "    return len(humans), len(chairs), human_on_chair_count\n",
        "\n",
        "# Initialize statistics\n",
        "total_humans = 0\n",
        "total_chairs = 0\n",
        "total_humans_on_chairs = 0\n",
        "total_images_processed = 0\n",
        "\n",
        "# Process images in each input directory separately\n",
        "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
        "for dir_name, input_dir in input_dirs.items():\n",
        "    dir_total_humans = 0\n",
        "    dir_total_chairs = 0\n",
        "    dir_total_humans_on_chairs = 0\n",
        "    image_paths = [os.path.join(input_dir, img) for img in os.listdir(input_dir) if img.endswith(image_extensions)]\n",
        "    for image_path in tqdm(image_paths, desc=f\"Processing images in {dir_name}\"):\n",
        "        humans, chairs, humans_on_chairs = plot_and_save_image_with_bboxes(image_path)\n",
        "        dir_total_humans += humans\n",
        "        dir_total_chairs += chairs\n",
        "        dir_total_humans_on_chairs += humans_on_chairs\n",
        "        total_images_processed += 1\n",
        "\n",
        "    # Print statistics for the current directory\n",
        "    print(f\"\\nResults for {dir_name}:\")\n",
        "    print(f\"Total images processed: {len(image_paths)}\")\n",
        "    print(f\"Total humans detected: {dir_total_humans}\")\n",
        "    print(f\"Total chairs detected: {dir_total_chairs}\")\n",
        "    print(f\"Total humans on chairs detected: {dir_total_humans_on_chairs}\")\n",
        "\n",
        "    # Update overall statistics\n",
        "    total_humans += dir_total_humans\n",
        "    total_chairs += dir_total_chairs\n",
        "    total_humans_on_chairs += dir_total_humans_on_chairs\n",
        "\n",
        "# Print final statistics\n",
        "print(\"\\nOverall Results:\")\n",
        "print(f\"Total images processed: {total_images_processed}\")\n",
        "print(f\"Total humans detected: {total_humans}\")\n",
        "print(f\"Total chairs detected: {total_chairs}\")\n",
        "print(f\"Total humans on chairs detected: {total_humans_on_chairs}\")\n",
        "\n",
        "print(\"Processing completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfbCMLFa0uNS"
      },
      "outputs": [],
      "source": [
        "total_images = len(raw_images)\n",
        "total_train_images = len(train_images)\n",
        "total_val_images = len(val_images)\n",
        "print(f\"Total images: {total_images}\")\n",
        "print(f\"Total training images: {total_train_images}\")\n",
        "print(f\"Total validation images: {total_val_images}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "# Load YOLOv5 large model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5l')\n",
        "\n",
        "# Define paths for each input directory\n",
        "input_dirs = {\n",
        "    'GAN/Real': '/content/drive/My Drive/GAN/Real',\n",
        "    'Dall_E': '/content/drive/My Drive/Dall_E',\n",
        "    'Stable_Diffuser': '/content/drive/My Drive/stable_difuser_images'\n",
        "}\n",
        "new_model_folder = '/content/drive/My Drive/processed_images/verification_model'\n",
        "images_folder = os.path.join(new_model_folder, 'images')\n",
        "annotations_folder = os.path.join(new_model_folder, 'annotations')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(images_folder, exist_ok=True)\n",
        "os.makedirs(annotations_folder, exist_ok=True)\n",
        "\n",
        "# Define relevant classes for objects that a human can sit on next to a pool\n",
        "relevant_classes = [0, 56, 57, 58]  # Include humans and objects they can sit on\n",
        "\n",
        "# Function to convert YOLO bbox to normal bbox\n",
        "def yolo_to_bbox(yolo_bbox, img_width, img_height):\n",
        "    x_center, y_center, width, height = yolo_bbox\n",
        "    x = int((x_center - width / 2) * img_width)\n",
        "    y = int((y_center - height / 2) * img_height)\n",
        "    w = int(width * img_width)\n",
        "    h = int(height * img_height)\n",
        "    return [x, y, w, h]\n",
        "\n",
        "# Function to convert normal bbox to YOLO bbox\n",
        "def bbox_to_yolo(bbox, img_width, img_height):\n",
        "    x, y, w, h = bbox\n",
        "    x_center = (x + w / 2) / img_width\n",
        "    y_center = (y + h / 2) / img_height\n",
        "    width = w / img_width\n",
        "    height = h / img_height\n",
        "    return [x_center, y_center, width, height]\n",
        "\n",
        "# Function to check if a human is on a chair\n",
        "def is_human_on_chair(human_bbox, chair_bbox):\n",
        "    hx, hy, hw, hh = human_bbox\n",
        "    cx, cy, cw, ch = chair_bbox\n",
        "    # Check if the center of the human is within the chair bounding box\n",
        "    return (cx <= hx + hw / 2 <= cx + cw) and (cy <= hy + hh / 2 <= cy + ch)\n",
        "\n",
        "# Function to detect objects using YOLOv5 model\n",
        "def detect_objects(image_path):\n",
        "    results = model(image_path)\n",
        "    return results.xyxy[0].cpu().numpy()\n",
        "\n",
        "# Function to plot bounding boxes on an example image and save it\n",
        "def plot_and_save_image_with_bboxes(image_path):\n",
        "    image_name = os.path.basename(image_path)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Failed to read image: {image_path}\")\n",
        "        return 0, 0, 0\n",
        "\n",
        "    img_height, img_width = img.shape[:2]\n",
        "\n",
        "    detections = detect_objects(image_path)\n",
        "    humans = []\n",
        "    chairs = []\n",
        "\n",
        "    for det in detections:\n",
        "        class_id, x1, y1, x2, y2, confidence = int(det[5]), int(det[0]), int(det[1]), int(det[2]), int(det[3]), float(det[4])\n",
        "        bbox = [x1, y1, x2 - x1, y2 - y1]\n",
        "        if class_id == 0:\n",
        "            humans.append(bbox)\n",
        "        elif class_id in relevant_classes:\n",
        "            chairs.append(bbox)\n",
        "\n",
        "    human_on_chair_count = 0\n",
        "    new_annotations = []\n",
        "\n",
        "    for human_bbox in humans:\n",
        "        for chair_bbox in chairs:\n",
        "            if is_human_on_chair(human_bbox, chair_bbox):\n",
        "                human_on_chair_count += 1\n",
        "                cv2.rectangle(img, (human_bbox[0], human_bbox[1]), (human_bbox[0] + human_bbox[2], human_bbox[1] + human_bbox[3]), (255, 0, 0), 3)\n",
        "                cv2.putText(img, 'Human on Chair', (human_bbox[0], human_bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "                # Save the new annotation for human on chair\n",
        "                new_annotations.append((2, bbox_to_yolo(human_bbox, img_width, img_height)))\n",
        "                break\n",
        "\n",
        "    if human_on_chair_count > 0:\n",
        "        # Save the image with bounding boxes and annotations\n",
        "        save_path = os.path.join(images_folder, image_name)\n",
        "        cv2.imwrite(save_path, img)\n",
        "\n",
        "        # Save the new annotations for human on chair\n",
        "        annotation_save_path = os.path.join(annotations_folder, image_name.replace('.jpg', '.txt'))\n",
        "        with open(annotation_save_path, 'w') as f:\n",
        "            for class_id, yolo_bbox in new_annotations:\n",
        "                f.write(f\"{class_id} {' '.join(map(str, yolo_bbox))}\\n\")\n",
        "\n",
        "    return len(humans), len(chairs), human_on_chair_count\n",
        "\n",
        "# Initialize statistics\n",
        "total_humans = 0\n",
        "total_chairs = 0\n",
        "total_humans_on_chairs = 0\n",
        "total_images_processed = 0\n",
        "\n",
        "# Process images in each input directory separately\n",
        "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
        "for dir_name, input_dir in input_dirs.items():\n",
        "    dir_total_humans = 0\n",
        "    dir_total_chairs = 0\n",
        "    dir_total_humans_on_chairs = 0\n",
        "    image_paths = [os.path.join(input_dir, img) for img in os.listdir(input_dir) if img.endswith(image_extensions)]\n",
        "    for image_path in tqdm(image_paths, desc=f\"Processing images in {dir_name}\"):\n",
        "        humans, chairs, humans_on_chairs = plot_and_save_image_with_bboxes(image_path)\n",
        "        dir_total_humans += humans\n",
        "        dir_total_chairs += chairs\n",
        "        dir_total_humans_on_chairs += humans_on_chairs\n",
        "        total_images_processed += 1\n",
        "\n",
        "    # Print statistics for the current directory\n",
        "    print(f\"\\nResults for {dir_name}:\")\n",
        "    print(f\"Total images processed: {len(image_paths)}\")\n",
        "    print(f\"Total humans detected: {dir_total_humans}\")\n",
        "    print(f\"Total chairs detected: {dir_total_chairs}\")\n",
        "    print(f\"Total humans on chairs detected: {dir_total_humans_on_chairs}\")\n",
        "\n",
        "    # Update overall statistics\n",
        "    total_humans += dir_total_humans\n",
        "    total_chairs += dir_total_chairs\n",
        "    total_humans_on_chairs += dir_total_humans_on_chairs\n",
        "\n",
        "# Print final statistics\n",
        "print(\"\\nOverall Results:\")\n",
        "print(f\"Total images processed: {total_images_processed}\")\n",
        "print(f\"Total humans detected: {total_humans}\")\n",
        "print(f\"Total chairs detected: {total_chairs}\")\n",
        "print(f\"Total humans on chairs detected: {total_humans_on_chairs}\")\n",
        "\n",
        "print(\"Processing completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBMeRo44CPwW",
        "outputId": "e4f4256d-9317-4bf1-92d2-8160262e1fd3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0'] not found, attempting AutoUpdate...\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 5.6 MB/s eta 0:00:00\n",
            "Collecting pillow>=10.3.0\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 25.9 MB/s eta 0:00:00\n",
            "Collecting requests>=2.32.0\n",
            "  Downloading requests-2.32.2-py3-none-any.whl (63 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.9/63.9 kB 195.2 MB/s eta 0:00:00\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 200.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, requests, pillow, gitdb, gitpython\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pillow-10.3.0 requests-2.32.2 smmap-5.0.1\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 10.2s, installed 3 packages: ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 2024-5-26 Python-3.10.12 torch-2.3.0+cu121 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt to yolov5l.pt...\n",
            "100%|██████████| 89.3M/89.3M [00:00<00:00, 348MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n",
            "Adding AutoShape... \n",
            "Processing images in GAN/Real: 100%|██████████| 372/372 [11:23<00:00,  1.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for GAN/Real:\n",
            "Total images processed: 372\n",
            "Total humans detected: 945\n",
            "Total chairs detected: 2121\n",
            "Total humans on chairs detected: 75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images in Dall_E: 100%|██████████| 131/131 [04:55<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for Dall_E:\n",
            "Total images processed: 131\n",
            "Total humans detected: 144\n",
            "Total chairs detected: 197\n",
            "Total humans on chairs detected: 41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images in Stable_Diffuser: 100%|██████████| 1064/1064 [39:13<00:00,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for Stable_Diffuser:\n",
            "Total images processed: 1064\n",
            "Total humans detected: 861\n",
            "Total chairs detected: 7296\n",
            "Total humans on chairs detected: 89\n",
            "\n",
            "Overall Results:\n",
            "Total images processed: 1567\n",
            "Total humans detected: 1950\n",
            "Total chairs detected: 9614\n",
            "Total humans on chairs detected: 205\n",
            "Processing completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "# Load YOLOv5 large model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5l')\n",
        "\n",
        "# Define paths for each input directory\n",
        "input_dirs = {\n",
        "    'GAN/Real': '/content/drive/My Drive/GAN/Real',\n",
        "    'Dall_E': '/content/drive/My Drive/Dall_E',\n",
        "    'Stable_Diffuser': '/content/drive/My Drive/stable_difuser_images'\n",
        "}\n",
        "new_model_folder = '/content/drive/My Drive/processed_images/verification_model'\n",
        "images_folder = os.path.join(new_model_folder, 'images')\n",
        "annotations_folder = os.path.join(new_model_folder, 'annotations')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(images_folder, exist_ok=True)\n",
        "os.makedirs(annotations_folder, exist_ok=True)\n",
        "\n",
        "# Define relevant classes for objects that a human can sit on next to a pool\n",
        "relevant_classes = [0, 56, 57, 58]  # Include humans and objects they can sit on\n",
        "\n",
        "# Function to convert YOLO bbox to normal bbox\n",
        "def yolo_to_bbox(yolo_bbox, img_width, img_height):\n",
        "    x_center, y_center, width, height = yolo_bbox\n",
        "    x = int((x_center - width / 2) * img_width)\n",
        "    y = int((y_center - height / 2) * img_height)\n",
        "    w = int(width * img_width)\n",
        "    h = int(height * img_height)\n",
        "    return [x, y, w, h]\n",
        "\n",
        "# Function to convert normal bbox to YOLO bbox\n",
        "def bbox_to_yolo(bbox, img_width, img_height):\n",
        "    x, y, w, h = bbox\n",
        "    x_center = (x + w / 2) / img_width\n",
        "    y_center = (y + h / 2) / img_height\n",
        "    width = w / img_width\n",
        "    height = h / img_height\n",
        "    return [x_center, y_center, width, height]\n",
        "\n",
        "# Function to check if a human is on a chair\n",
        "def is_human_on_chair(human_bbox, chair_bbox):\n",
        "    hx, hy, hw, hh = human_bbox\n",
        "    cx, cy, cw, ch = chair_bbox\n",
        "    # Check if the center of the human is within the chair bounding box\n",
        "    return (cx <= hx + hw / 2 <= cx + cw) and (cy <= hy + hh / 2 <= cy + ch)\n",
        "\n",
        "# Function to detect objects using YOLOv5 model\n",
        "def detect_objects(image_path):\n",
        "    results = model(image_path)\n",
        "    return results.xyxy[0].cpu().numpy()\n",
        "\n",
        "# Function to plot bounding boxes on an example image and save it\n",
        "def plot_and_save_image_with_bboxes(image_path):\n",
        "    image_name = os.path.basename(image_path)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Failed to read image: {image_path}\")\n",
        "        return 0, 0, 0\n",
        "\n",
        "    img_height, img_width = img.shape[:2]\n",
        "\n",
        "    detections = detect_objects(image_path)\n",
        "    humans = []\n",
        "    chairs = []\n",
        "\n",
        "    for det in detections:\n",
        "        class_id, x1, y1, x2, y2, confidence = int(det[5]), int(det[0]), int(det[1]), int(det[2]), int(det[3]), float(det[4])\n",
        "        bbox = [x1, y1, x2 - x1, y2 - y1]\n",
        "        if class_id == 0:\n",
        "            humans.append(bbox)\n",
        "        elif class_id in relevant_classes:\n",
        "            chairs.append(bbox)\n",
        "\n",
        "    human_on_chair_count = 0\n",
        "    new_annotations = []\n",
        "\n",
        "    for human_bbox in humans:\n",
        "        for chair_bbox in chairs:\n",
        "            if is_human_on_chair(human_bbox, chair_bbox):\n",
        "                human_on_chair_count += 1\n",
        "                cv2.rectangle(img, (human_bbox[0], human_bbox[1]), (human_bbox[0] + human_bbox[2], human_bbox[1] + human_bbox[3]), (255, 0, 0), 3)\n",
        "                cv2.putText(img, 'Human on Chair', (human_bbox[0], human_bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "                # Save the new annotation for human on chair\n",
        "                new_annotations.append((2, bbox_to_yolo(human_bbox, img_width, img_height)))\n",
        "                break\n",
        "\n",
        "    if human_on_chair_count > 0:\n",
        "        # Save the image with bounding boxes and annotations\n",
        "        save_path = os.path.join(images_folder, image_name)\n",
        "        cv2.imwrite(save_path, img)\n",
        "\n",
        "        # Save the new annotations for human on chair\n",
        "        annotation_save_path = os.path.join(annotations_folder, image_name.replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt'))\n",
        "        with open(annotation_save_path, 'w') as f:\n",
        "            for class_id, yolo_bbox in new_annotations:\n",
        "                f.write(f\"{class_id} {' '.join(map(str, yolo_bbox))}\\n\")\n",
        "\n",
        "    return len(humans), len(chairs), human_on_chair_count\n",
        "\n",
        "# Initialize statistics\n",
        "total_humans = 0\n",
        "total_chairs = 0\n",
        "total_humans_on_chairs = 0\n",
        "total_images_processed = 0\n",
        "\n",
        "# Process images in each input directory separately\n",
        "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
        "for dir_name, input_dir in input_dirs.items():\n",
        "    dir_total_humans = 0\n",
        "    dir_total_chairs = 0\n",
        "    dir_total_humans_on_chairs = 0\n",
        "    image_paths = [os.path.join(input_dir, img) for img in os.listdir(input_dir) if img.endswith(image_extensions)]\n",
        "    for image_path in tqdm(image_paths, desc=f\"Processing images in {dir_name}\"):\n",
        "        humans, chairs, humans_on_chairs = plot_and_save_image_with_bboxes(image_path)\n",
        "        dir_total_humans += humans\n",
        "        dir_total_chairs += chairs\n",
        "        dir_total_humans_on_chairs += humans_on_chairs\n",
        "        total_images_processed += 1\n",
        "\n",
        "    # Print statistics for the current directory\n",
        "    print(f\"\\nResults for {dir_name}:\")\n",
        "    print(f\"Total images processed: {len(image_paths)}\")\n",
        "    print(f\"Total humans detected: {dir_total_humans}\")\n",
        "    print(f\"Total chairs detected: {dir_total_chairs}\")\n",
        "    print(f\"Total humans on chairs detected: {dir_total_humans_on_chairs}\")\n",
        "\n",
        "    # Update overall statistics\n",
        "    total_humans += dir_total_humans\n",
        "    total_chairs += dir_total_chairs\n",
        "    total_humans_on_chairs += dir_total_humans_on_chairs\n",
        "\n",
        "# Print final statistics\n",
        "print(\"\\nOverall Results:\")\n",
        "print(f\"Total images processed: {total_images_processed}\")\n",
        "print(f\"Total humans detected: {total_humans}\")\n",
        "print(f\"Total chairs detected: {total_chairs}\")\n",
        "print(f\"Total humans on chairs detected: {total_humans_on_chairs}\")\n",
        "\n",
        "print(\"Processing completed.\")\n"
      ],
      "metadata": {
        "id": "_2mYuPJ2WfC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that each image has a corresponding annotation file\n",
        "missing_annotations = []\n",
        "\n",
        "for img in os.listdir(images_folder):\n",
        "    if img.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        base_name = os.path.splitext(img)[0]\n",
        "        annotation_file = base_name + '.txt'\n",
        "        if not os.path.exists(os.path.join(annotations_folder, annotation_file)):\n",
        "            missing_annotations.append(img)\n",
        "\n",
        "if missing_annotations:\n",
        "    print(\"Images without corresponding annotations:\")\n",
        "    for img in missing_annotations:\n",
        "        print(img)\n",
        "else:\n",
        "    print(\"All images have corresponding annotations.\")\n"
      ],
      "metadata": {
        "id": "wF1sXVdQWfdn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}